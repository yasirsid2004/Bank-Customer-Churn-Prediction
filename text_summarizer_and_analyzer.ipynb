{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOCAkMZ1pbrjGXWJ6DZjZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasirsid2004/Bank-Customer-Churn-Prediction/blob/main/text_summarizer_and_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required libraries quietly\n",
        "!pip install transformers torch sentencepiece nltk --quiet\n",
        "\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "import textwrap\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Step 2: Download necessary NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpus/stopwords')\n",
        "    nltk.data.find('tokenizers/punkt_tab') # Check for punkt_tab as well\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK data (one-time setup)...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True) # Download punkt_tab\n",
        "    print(\"NLTK data downloaded.\")\n",
        "\n",
        "\n",
        "# Step 3: Define the core functions\n",
        "def analyze_text(text):\n",
        "    \"\"\"Performs a basic analysis of the given text.\"\"\"\n",
        "    # Word count (handles various spacing)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    word_count = len(words)\n",
        "\n",
        "    # Character count\n",
        "    char_count = len(text)\n",
        "\n",
        "    # Sentence count\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Estimated reading time (average 200 words per minute)\n",
        "    reading_time = round(word_count / 200)\n",
        "    if reading_time < 1:\n",
        "        reading_time = \"less than 1\"\n",
        "\n",
        "\n",
        "    # Keyword Extraction (most common words, excluding stopwords)\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "    word_freq = Counter(filtered_words)\n",
        "    keywords = [word for word, freq in word_freq.most_common(5)]\n",
        "\n",
        "    return {\n",
        "        \"word_count\": word_count,\n",
        "        \"char_count\": char_count,\n",
        "        \"sentence_count\": sentence_count,\n",
        "        \"reading_time_minutes\": reading_time,\n",
        "        \"keywords\": keywords\n",
        "    }\n",
        "\n",
        "def summarize_text(text):\n",
        "    \"\"\"Summarizes the text using a pre-trained Hugging Face model.\"\"\"\n",
        "    print(\"\\nInitializing summarization model (this may take a moment on first run)...\")\n",
        "    try:\n",
        "        # Using a distilled, faster model perfect for Colab\n",
        "        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "        # The model has a max input length; we'll truncate if necessary.\n",
        "        # A more advanced approach would be to chunk the text.\n",
        "        max_chunk_length = 1024\n",
        "\n",
        "        # Summarize with min/max length constraints\n",
        "        summary = summarizer(text[:max_chunk_length], max_length=150, min_length=40, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during summarization: {e}\"\n",
        "\n",
        "def chatbot():\n",
        "    \"\"\"Main function to run the interactive chatbot.\"\"\"\n",
        "    wrapper = textwrap.TextWrapper(width=80)\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸš€ Welcome to the Text Analyzer and Summarizer Chatbot! ðŸš€\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nPaste the text you want to analyze and summarize below.\")\n",
        "        print(\"When you're finished, press Enter on an empty line.\")\n",
        "\n",
        "        # Collect multi-line input from the user\n",
        "        user_text_lines = []\n",
        "        while True:\n",
        "            line = input()\n",
        "            if line == \"\":\n",
        "                break\n",
        "            user_text_lines.append(line)\n",
        "\n",
        "        user_text = \"\\n\".join(user_text_lines)\n",
        "\n",
        "        if not user_text.strip():\n",
        "            print(\"\\nIt looks like you didn't enter any text. Let's try again!\")\n",
        "            continue\n",
        "\n",
        "        # --- Analysis ---\n",
        "        print(\"\\n\" + \"-\"*35 + \" ANALYSIS \" + \"-\"*35)\n",
        "        analysis_results = analyze_text(user_text)\n",
        "        print(f\"   Word Count: {analysis_results['word_count']}\")\n",
        "        print(f\"   Character Count: {analysis_results['char_count']}\")\n",
        "        print(f\"   Sentence Count: {analysis_results['sentence_count']}\")\n",
        "        print(f\"   Estimated Reading Time: {analysis_results['reading_time_minutes']} minute(s)\")\n",
        "        print(f\"   Top Keywords: {', '.join(analysis_results['keywords'])}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # --- Summarization ---\n",
        "        summary = summarize_text(user_text)\n",
        "        print(\"\\n\" + \"*\"*35 + \" SUMMARY \" + \"*\"*36)\n",
        "        print(wrapper.fill(text=summary))\n",
        "        print(\"*\" * 80)\n",
        "\n",
        "        # --- Continue or Exit ---\n",
        "        while True:\n",
        "            another = input(\"\\nWould you like to analyze another text? (yes/no): \").lower()\n",
        "            if another in [\"yes\", \"y\", \"no\", \"n\"]:\n",
        "                break\n",
        "            print(\"Invalid input. Please enter 'yes' or 'no'.\")\n",
        "\n",
        "        if another in [\"no\", \"n\"]:\n",
        "            print(\"\\nThank you for using the chatbot. Goodbye! ðŸ‘‹\")\n",
        "            break\n",
        "\n",
        "# Start the chatbot\n",
        "chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JCefbOdEgRO",
        "outputId": "2496b13a-6f5a-4751-e91c-56f59d44c316"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noDownloading NLTK data (one-time setup)...\n",
            "NLTK data downloaded.\n",
            "================================================================================\n",
            "ðŸš€ Welcome to the Text Analyzer and Summarizer Chatbot! ðŸš€\n",
            "================================================================================\n",
            "\n",
            "Paste the text you want to analyze and summarize below.\n",
            "When you're finished, press Enter on an empty line.\n",
            "Iâ€™m an aspiring AI and Machine Learning engineer driven by curiosity, creativity, and a desire to turn data into meaningful impact. With a strong foundation in Python and hands-on experience exploring intelligent systems, I aim to design solutions that blend innovation with purpose. My goal is to grow as a problem-solver who not only codes but also creates â€” developing technologies that inspire progress and make everyday life smarter.\n",
            "\n",
            "\n",
            "----------------------------------- ANALYSIS -----------------------------------\n",
            "   Word Count: 71\n",
            "   Character Count: 438\n",
            "   Sentence Count: 3\n",
            "   Estimated Reading Time: less than 1 minute(s)\n",
            "   Top Keywords: aspiring, machine, learning, engineer, driven\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Initializing summarization model (this may take a moment on first run)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*********************************** SUMMARY ************************************\n",
            " I'm an aspiring AI and Machine Learning engineer driven by curiosity,\n",
            "creativity, and a desire to turn data into meaningful impact . My goal is to\n",
            "grow as a problem-solver who not only codes but also creates â€” developing\n",
            "technologies that inspire progress and make everyday life smarter .\n",
            "********************************************************************************\n",
            "\n",
            "Would you like to analyze another text? (yes/no): no\n",
            "\n",
            "Thank you for using the chatbot. Goodbye! ðŸ‘‹\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpus/stopwords')\n",
        "    print(\"âœ… NLTK data is already downloaded.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK data...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    print(\"âœ… NLTK data downloaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5zbTel0Eo-p",
        "outputId": "879a20e0-0768-4950-8a49-ca32e1f6dfff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data...\n",
            "âœ… NLTK data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_text(text):\n",
        "    \"\"\"Performs a basic analysis of the given text.\"\"\"\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    word_count = len(words)\n",
        "    char_count = len(text)\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    reading_time = round(word_count / 200)\n",
        "    if reading_time < 1:\n",
        "        reading_time = \"less than 1\"\n",
        "\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "    word_freq = Counter(filtered_words)\n",
        "    keywords = [word for word, freq in word_freq.most_common(5)]\n",
        "\n",
        "    return {\n",
        "        \"word_count\": word_count,\n",
        "        \"char_count\": char_count,\n",
        "        \"sentence_count\": sentence_count,\n",
        "        \"reading_time_minutes\": reading_time,\n",
        "        \"keywords\": keywords\n",
        "    }\n",
        "\n",
        "def summarize_text(text):\n",
        "    \"\"\"Summarizes the text using a pre-trained Hugging Face model.\"\"\"\n",
        "    print(\"\\nInitializing summarization model (this may take a moment on the first run)...\")\n",
        "    try:\n",
        "        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "        max_chunk_length = 1024\n",
        "        summary = summarizer(text[:max_chunk_length], max_length=150, min_length=40, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during summarization: {e}\"\n",
        "\n",
        "print(\"âœ… Core functions (analyze_text, summarize_text) are defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD2hKfR3E06P",
        "outputId": "c15a0ace-27f6-47ca-cad7-012e93f2c50d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Core functions (analyze_text, summarize_text) are defined.\n"
          ]
        }
      ]
    }
  ]
}